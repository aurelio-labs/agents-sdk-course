{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Agents SDK Course](https://www.aurelio.ai/course/agents-sdk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/agents-sdk-course/blob/main/chapters/10-rag-agent.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/aurelio-labs/agents-sdk-course/blob/main/chapters/10-rag-agent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Agent\n",
    "\n",
    "**R**etrieval **A**ugmented **G**eneration (RAG) is a powerful technique that enables AI agents to access and leverage external knowledge sources beyond their training data. In this tutorial, we'll build a RAG agent that can answer questions about the JFK assassination files using OpenAI's Agents SDK and Pinecone vector database.\n",
    "\n",
    "RAG is particularly useful when:\n",
    "- You need up-to-date information beyond the model's training cutoff\n",
    "- You have domain-specific documents or proprietary data\n",
    "- You want to reduce hallucinations by grounding responses in factual sources\n",
    "- You need to cite sources for transparency and verification\n",
    "\n",
    "By the end of this tutorial, you'll have built an agent that can search through historical documents and provide accurate, sourced answers about the JFK files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before we begin, let's install the required packages:\n",
    "\n",
    "```bash\n",
    "!pip install -qU \\\n",
    "    openai-agents==0.0.13 \\\n",
    "    pinecone==7.0.2 \\\n",
    "    datasets==3.6.0 \\\n",
    "    semantic-chunkers==0.1.1\n",
    "```\n",
    "\n",
    "We'll also need API keys for OpenAI and Pinecone. You can get:\n",
    "- An OpenAI API key from the [OpenAI Platform](https://platform.openai.com/api-keys)\n",
    "- A Pinecone API key from the [Pinecone Console](https://app.pinecone.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"Enter OPENAI_API_KEY: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LLM Knowledge Limitations\n",
    "\n",
    "Before implementing RAG, let's first demonstrate why it's needed. We'll create a basic agent and test its knowledge about specific topics to show the limitations of relying solely on the model's training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Agent\",\n",
    "    model=\"gpt-4.1-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll ask our agent `\"where was Oswald in october 1959?\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Runner\n",
    "\n",
    "query = \"where was Lee Harvey Oswald in october 1959?\"\n",
    "\n",
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=query,\n",
    ")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oswald was _also_ in Helsinki, Finland in October 1959 according to the [JFK files](https://www.archives.gov/files/research/jfk/releases/2025/0318/104-10004-10156.pdf) - which our agent missed. We can try and tease out this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "        {\"role\": \"assistant\", \"content\": result.final_output},\n",
    "        {\"role\": \"user\", \"content\": \"did he go anywhere else?\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent is clearly not aware of Oswald's trip to Helsinki - that is because the underlying LLM has not seen that information during it's training process. We call information learned during LLM training **parametric knowledge**, ie knowledge stored within the model _parameters_.\n",
    "\n",
    "LLMs can also make use of **source knowledge** to answer questions. Source knowledge refers to information provided to an LLM via a prompt, either provided via the user, the LLM instructions, or in our case - via an external database - ie with **R**etrieval **A**ugmented **G**eneration (RAG). Before we build out our RAG pipeline, let's see if our LLM can answer our question when we provide the relevant information about Oswald's whereabouts via our `instructions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_knowledge = (\n",
    "    \"~SECRET~\\n\"\n",
    "    \"1 June 1964\\n\"\n",
    "    \"\\n\"\n",
    "    \"## MEMO FOR THE RECORD\\n\"\n",
    "    \"\\n\"\n",
    "    \"1. At 0900 this morning I talked with Frank Friberg recently \"\n",
    "    \"returned COS Helsinki re Warren Commission inquiry concerning \"\n",
    "    \"the timetable of Oswald's stay in Finland in October 1959, including \"\n",
    "    \"his contact with the Soviet Consulate there. (Copy of the Commission \"\n",
    "    \"letter of 25 May 64 and State Cable of 22 May 64 attached.)\"\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Agent\",\n",
    "    instructions=(\n",
    "        \"You are an assistant specialized in answering questions about the JFK assassination\"\n",
    "        \"and related documents.\\n\"\n",
    "        \"Here is some additional context that you can use:\\n\"\n",
    "        f\"{source_knowledge}\\n\"\n",
    "    ),\n",
    "    model=\"gpt-4.1-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask our original `query` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=query,\n",
    ")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, this is much better! Now what we just did works for this simple example, but it doesn't scale. If we want an agent that can answer any question and use context from _all_ of the JFK files, we need to build a RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RAG pipeline actually requires two _core_ pipelines - an **ingestion pipeline** and a **retrieval pipeline**. At a high level those pipelines are responsible for:\n",
    "\n",
    "* **Ingestion** handles the initial data preparation, embedding, and indexing. We'll explain those steps in more detail soon, but the tldr is that the ingestion pipeline will transform a set of unstructured and messy PDFs into a \"second brain\" for our agent, ie the _source knowledge_.\n",
    "\n",
    "* **Retrieval** handles the query-time retrieval of information. It defines how we access and retrieve source knowledge from our second brain.\n",
    "\n",
    "Naturally, we need to first develop our **ingestion pipeline** so that we can populate our second brain before we use the **retrieval pipeline** to retrieve anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ingestion pipeline consists of three (or four) steps:\n",
    "\n",
    "0. **Process the PDF** into plain text - with the `aurelio-ai/jfk-files` dataset (below) this step has been completed.\n",
    "\n",
    "1. **Chunk** the plain text into smaller segments (a good rule of thumb is ~300-400 tokens per chunk).\n",
    "\n",
    "2. **Embed** each chunk with OpenAI's `text-embedding-3-small` to create _vectors_.\n",
    "\n",
    "3. **Index** those vectors in Pinecone with metadata like _source URL_, _document title_, etc.\n",
    "\n",
    "![JFK document ingestion pipeline, covering PDF text to chunked text, embedding those chunks into semantically meaningful vector embeddings, and sending those vector embeddings to a vector database](../assets/jfk-ingestion-pipeline.png)\n",
    "\n",
    "To begin, we'll start at step **0** and download the pre-parsed JFK files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loading the JFK Files Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a dataset of the JFK files, which we will pull from the [Hugging Face Hub](). This dataset contains historical documents that our agent will search through to answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"aurelio-ai/jfk-files\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a sample document to understand the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document contains:\n",
    "- `id`: Unique identifier for the document\n",
    "- `filename`: Name of the PDF file\n",
    "- `url`: Link to the original document\n",
    "- `date`: Publication date\n",
    "- `content`: The full text content\n",
    "- `pages`: Number of pages in the original document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step **1** in our ingestion pipeline is to _chunk_ our dataset. As mentioned we will be splitting each PDF into chunks of ~400 tokens. We'll also handle cases where a PDF contains little-to-no information by not indexing that PDF, and cases where our final chunk is too small to be relevant by appending it to the previous chunk.\n",
    "\n",
    "We use the lightweight [`semantic-chunkers`](https://github.com/aurelio-labs/semantic-chunkers) library and a simple `RegexChunker` for chunking. We will set the token limit for each chunk to `400` tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunkers import RegexChunker\n",
    "\n",
    "chunker = RegexChunker(max_chunk_tokens=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chunk a doc like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunker(docs=[dataset[1]['content']])\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outputs a list of a list of `Chunk` objects. These `Chunk` objects contain many smaller `splits`, which can be thought of as chunks within chunks. We can view the chunks in a cleaner way using `chunker.print` on a `list[Chunk]` object like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker.print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the text content from our chunks which we access via the `content` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0][0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we'll setup our Pinecone vector DB and begin **embedding _and_ indexing** our data in one step - while indexing we'll be performing the above chunking logic across all our docs before they're embedded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To enable semantic search over our documents, we'll use [Pinecone](https://www.pinecone.io/) - a fully managed vector database. Vector databases allow us to store and search through vector embeddings (numerical representations of text) to find semantically similar content. There are many vector DB options out there, alongside Pinecone we also recommend [Qdrant](https://qdrant.tech/) and [pgvector](https://github.com/pgvector/pgvector).\n",
    "\n",
    "First, let's set up our Pinecone API key which we can find in the [Pinecone console](https://app.pinecone.io/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\") or getpass(\n",
    "    \"Enter PINECONE_API_KEY: \"\n",
    ")\n",
    "\n",
    "# initializing the pinecone client\n",
    "pc = Pinecone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a Pinecone index to store our vector embeddings. We specify the following:\n",
    "\n",
    "- We want to use AWS via `cloud=CloudProvider.AWS` in Pinecone's free tier region via `region=AwsRegion.US_EAST_1`.\n",
    "- We use the `llama-text-embed-v2` embedding model hosted by Pinecone - by default the index will be configured for this model.\n",
    "- We specify that the text content that should be embedding by our model will be provided to Pinecone via the `content` metadata field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import AwsRegion, CloudProvider\n",
    "\n",
    "# set our index name, you can change this to whatever you like\n",
    "index_name = \"agents-sdk-course-jfk-files\"\n",
    "\n",
    "# if the index doesn't exist, create it\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index_for_model(\n",
    "        name=index_name,\n",
    "        cloud=CloudProvider.AWS,\n",
    "        region=AwsRegion.US_EAST_1,\n",
    "        embed={\n",
    "            \"model\": \"llama-text-embed-v2\",\n",
    "            \"field_map\": {\n",
    "                \"text\": \"content\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if our index is empty (it should be on first run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To embed and index a chunk, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = dataset[1]\n",
    "\n",
    "# chunk the doc\n",
    "chunks = chunker(docs=[doc['content']])\n",
    "\n",
    "# create a list of dictionary records\n",
    "records = [\n",
    "    {\n",
    "        \"id\": doc['id']+f\"-{i}\",\n",
    "        \"content\": chunk.content,\n",
    "        \"filename\": doc['filename'],\n",
    "        \"url\": doc['url'],\n",
    "        \"date\": doc['date'].isoformat(),\n",
    "        \"pages\": doc['pages']\n",
    "    } for i, chunk in enumerate(chunks[0])\n",
    "]\n",
    "\n",
    "# embed and index\n",
    "index.upsert_records(\n",
    "    namespace=\"default\",\n",
    "    records=records\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should see that our index contains three records inside the `default` namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now we simply repeat that process for all of our docs. We will do this in batches to avoid excessive network calls with small packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "records = []\n",
    "for doc in tqdm(dataset):\n",
    "    # perform a quick length check of our docs to avoid excessively small docs\n",
    "    if len(doc['content']) < 100:\n",
    "        # nothing less than 100 chars\n",
    "        continue\n",
    "    # chunk the docs\n",
    "    chunks = chunker(docs=[doc['content']])\n",
    "    for i, chunk in enumerate(chunks[0]):\n",
    "        records.append(\n",
    "            {\n",
    "                \"id\": doc['id']+f\"-{i}\",\n",
    "                \"content\": chunk.content,\n",
    "                \"filename\": doc['filename'],\n",
    "                \"url\": doc['url'],\n",
    "                \"date\": doc['date'].isoformat(),\n",
    "                \"pages\": doc['pages']\n",
    "            }\n",
    "        )\n",
    "    if len(records) >= 64:\n",
    "        # if we have a particularly long doc, we'll need to split up the batch\n",
    "        for i in range(0, len(records), 96):\n",
    "            # 96 is the max number of records we can upsert in one go\n",
    "            batch = records[i:i+96]\n",
    "            # embed and index the batch\n",
    "            index.upsert_records(\n",
    "                namespace=\"default\",\n",
    "                records=batch\n",
    "            )\n",
    "        records = []\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's our **ingestion pipeline** complete and we're ready to move on to the **retrieval pipeline**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our retrieval pipeline is what will be used to retrieve the right source knowledge for our agent at query-time. We will be implementing this via an Agent SDK `@function_tool` but before we do so let's directly test retrieval.\n",
    "\n",
    "As we're using Pinecone's integrated inference (ie both indexing and embedding are handled by Pinecone) the retrieval pipeline is incredibly simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = index.search(\n",
    "    namespace=\"default\",\n",
    "    query={\n",
    "        \"inputs\": {\"text\": query},\n",
    "        \"top_k\": 5\n",
    "    },\n",
    "    fields=[\"content\", \"url\", \"pages\"]\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's format these a little nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# let's print out the results\n",
    "results_str = \"\"\"\n",
    "| Score | Content | Pages | URL |\n",
    "|-------|---------|-------|-----|\n",
    "\"\"\"\n",
    "for result in results[\"result\"][\"hits\"]:\n",
    "    results_str += (\n",
    "        f\"| {result['_score']:.2f} \"\n",
    "        f\"| {result['fields']['content'].replace('|', '\\|')} \"\n",
    "        f\"| {result['fields']['pages']} \"\n",
    "        f\"| {result['fields']['url']} |\\n\"\n",
    "    )\n",
    "\n",
    "display(Markdown(results_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a tool that our agent can use to search through the JFK documents. We use the `@function_tool` decorator to wrap the logic above and make the retrieval pipeline available to our agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import function_tool\n",
    "\n",
    "@function_tool\n",
    "def jfk_files_search(query: str) -> str:\n",
    "    \"\"\"This tool gives you search access to the full JFK files. To use this tool you\n",
    "    should provide search queries with as much context as possible, and using natural\n",
    "    language to describe the query.\n",
    "\n",
    "    This tool will return five of the most relevant document chunks for your query,\n",
    "    including the result's similarity score, the text content, the source page number,\n",
    "    and source URL.\n",
    "    \"\"\"\n",
    "    results = index.search(\n",
    "        namespace=\"default\",\n",
    "        query={\n",
    "            \"inputs\": {\"text\": query},\n",
    "            \"top_k\": 5\n",
    "        },\n",
    "        fields=[\"content\", \"url\", \"pages\"]\n",
    "    )\n",
    "    # format the results into a markdown string - this isn't essential for our LLM but\n",
    "    # it helps\n",
    "    source_knowledge = \"\"\"\n",
    "    | Score | Content | Pages | URL |\n",
    "    |-------|---------|-------|-----|\n",
    "    \"\"\"\n",
    "    for result in results[\"result\"][\"hits\"]:\n",
    "        source_knowledge += (\n",
    "            f\"| {result['_score']:.2f} \"\n",
    "            f\"| {result['fields']['content'].replace('|', '\\|')} \"\n",
    "            f\"| {result['fields']['pages']} \"\n",
    "            f\"| {result['fields']['url']} |\\n\"\n",
    "        )\n",
    "    return source_knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we provide our `jfk_files_search` tool to an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"JFK Document Assistant\",\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    instructions=(\n",
    "        \"You are an assistant specialized in answering questions about the JFK \"\n",
    "        \"assassination and related documents. When users ask questions about JFK, \"\n",
    "        \"the assassination, or related historical events. Please write your answers \"\n",
    "        \"in markdown and provide sources to support your answers.\"\n",
    "    ),\n",
    "    tools=[jfk_files_search]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Final RAG Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our agent to discover who really assassinated JFK. First, let's confirm our agent is functional with our original query about Oswald's whereabouts in October 1959."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=query,\n",
    ")\n",
    "\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things conversational we'll append our own queries and the agent responses to a `messages` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "    {\"role\": \"assistant\", \"content\": result.final_output}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(\n",
    "    {\"role\": \"user\", \"content\": (\n",
    "        \"do the JFK files contain any information about doubts on Lee Harvey Oswald's \"\n",
    "        \"involvement in the assassination?\"\n",
    "    )}\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=messages,\n",
    ")\n",
    "\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.extend(\n",
    "    [\n",
    "        {\"role\": \"assistant\", \"content\": result.final_output},\n",
    "        {\"role\": \"user\", \"content\": \"I see mentions of Oswald in Mexico, what did he do there?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=messages,\n",
    ")\n",
    "\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.extend(\n",
    "    [\n",
    "        {\"role\": \"assistant\", \"content\": result.final_output},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me more about Valeriy, is he relevant?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=messages,\n",
    ")\n",
    "\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our retrieval pipeline is clearly returning highly relevant information to our agent - allowing us to explore the JFK files, as follow up questions, and try to understand the various connections and characters that appear throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we're done asking questions, we should ideally delete our vector index to save resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

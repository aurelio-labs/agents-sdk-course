{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/agents-sdk-course/blob/main/chapters/07-voice.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/aurelio-labs/agents-sdk-course/blob/main/chapters/07-voice.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Agents SDK Course](https://www.aurelio.ai/course/agents-sdk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice\n",
    "\n",
    "The Agents SDK introduces several unique features, with one of the standout capabilities being voice functionality. The voice tutorial demonstrates how to build voice-enabled AI agents that can process spoken input, generate intelligent responses, and deliver those responses as natural-sounding speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you're working in Google Colab or another remote notebook service, you can install the following requirements for this notebook. If running locally, refer to the `uv` setup instructions in the [README](../README.md).\n",
    "\n",
    "```\n",
    "!pip install -qU \\\n",
    "    \"matplotlib==3.10.1\" \\\n",
    "    \"openai==1.68.2\" \\\n",
    "    \"openai-agents[voice]==0.0.12\" \\\n",
    "    \"sounddevice==0.5.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Sound in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the [`sounddevice` library](https://python-sounddevice.readthedocs.io/) to handle the audio input and streaming — which allows us to record audio into a numpy array, and play audio from a numpy array.\n",
    "\n",
    "Before recording / playing audio with `sounddevice` we need to find the sample rate of our input and output devices. We can find our input / output device details using the `query_devices` function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "input_device = sd.query_devices(kind='input')\n",
    "output_device = sd.query_devices(kind='output')\n",
    "\n",
    "input_device, output_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the sample rate for these devices via the `default_samplerate` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_samplerate = sd.query_devices(kind='input')['default_samplerate']\n",
    "out_samplerate = sd.query_devices(kind='output')['default_samplerate']\n",
    "\n",
    "in_samplerate, out_samplerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can record a stream of audio via `sd.InputStream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_chunks = []\n",
    "\n",
    "# start streaming from microphone until Enter is pressed\n",
    "with sd.InputStream(\n",
    "    samplerate=in_samplerate,\n",
    "    channels=1,\n",
    "    dtype='int16',\n",
    "    callback=lambda indata, frames, time, status: recorded_chunks.append(indata.copy())\n",
    "):\n",
    "    input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `recorded_chunks` is a list of numpy arrays (the _chunks_) each containing audio data as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recorded_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these chunks contains a `512` element vector, representing the audio for that chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_chunks[0].shape, recorded_chunks[1].shape, recorded_chunks[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We `concatenate` these chunks to create a single audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "audio_buffer = np.concatenate(recorded_chunks)\n",
    "audio_buffer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play the audio back like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(audio_buffer, samplerate=out_samplerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `recorded_chunks` is a numpy array containing the recording we just made via `sd.InputStream` — we can visualize it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create time axis\n",
    "time = np.linspace(0, len(audio_buffer) / in_samplerate, num=len(audio_buffer))\n",
    "\n",
    "# Plot the waveform\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(time, audio_buffer)\n",
    "plt.title('Audio Waveform')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before we can pass this data to a voice pipeline in Agents SDK, we must transform it into an `AudioInput` object like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.voice import AudioInput\n",
    "\n",
    "audio_input = AudioInput(\n",
    "    buffer=audio_buffer,\n",
    "    frame_rate=in_samplerate,\n",
    "    channels=audio_buffer.shape[1],  # 1 channel == mono, 2 channels == stereo\n",
    ")\n",
    "audio_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the `VoicePipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen how to work with audio in Python. Now it's time for us to jump into working with audio with the Agents SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass.getpass(\"OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=(\n",
    "        \"Repeat the user's question back to them, and then answer it. Note that the user is \"\n",
    "        \"speaking to you via a voice interface, although you are reading and writing text to \"\n",
    "        \"respond. Nonetheless, ensure that your written response is easily translatable to voice.\"\n",
    "    ),\n",
    "    model=\"gpt-4.1-nano\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `VoicePipeline` from Agents SDK, which requires two parameters.\n",
    "\n",
    "### Workflow Parameter\n",
    "\n",
    "The `workflow` which is our `agent` from above transformed into a voice workflow via the `SingleAgentVoiceWorkflow` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.voice import SingleAgentVoiceWorkflow\n",
    "\n",
    "workflow = SingleAgentVoiceWorkflow(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Parameter\n",
    "\n",
    "The `config` is where we pass our `VoicePipelineConfig`. Inside this config we provide a `TTSModelSettings` object within which we provide instructions on how the voice should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.voice import TTSModelSettings, VoicePipelineConfig\n",
    "\n",
    "# Define custom TTS model settings with the desired instructions\n",
    "custom_tts_settings = TTSModelSettings(\n",
    "    instructions=(\n",
    "        \"Personality: upbeat, friendly, persuasive guide.\\n\"\n",
    "        \"Tone: Friendly, clear, and reassuring, creating a calm atmosphere and making \"\n",
    "        \"the listener feel confident and comfortable.\\n\"\n",
    "        \"Pronunciation: Clear, articulate, and steady, ensuring each instruction is \"\n",
    "        \"easily understood while maintaining a natural, conversational flow.\\n\"\n",
    "        \"Tempo: Speak relatively fast, include brief pauses and after before questions.\\n\"\n",
    "        \"Emotion: Warm and supportive, conveying empathy and care, ensuring the listener \"\n",
    "        \"feels guided and safe throughout the journey.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "voice_pipeline_config = VoicePipelineConfig(tts_settings=custom_tts_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.voice import VoicePipeline\n",
    "\n",
    "pipeline = VoicePipeline(workflow=workflow, config=voice_pipeline_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can provide our `audio_input` to our pipeline to receive an audio output stream. This is handled asynchronousely so we must `await` the pipeline and capture the audio streamed events, which we find via the `type=\"voice_stream_event_audio\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await pipeline.run(audio_input=audio_input)\n",
    "\n",
    "response_chunks = []\n",
    "\n",
    "async for event in result.stream():\n",
    "    if event.type == \"voice_stream_event_audio\":\n",
    "        response_chunks.append(event.data)\n",
    "\n",
    "# concatenate all of the chunks into a single audio buffer\n",
    "response_audio_buffer = np.concatenate(response_chunks, axis=0)\n",
    "\n",
    "# openai sample rate is 24000\n",
    "openai_sample_rate = 24_000\n",
    "\n",
    "# play the response\n",
    "sd.play(response_audio_buffer, samplerate=openai_sample_rate)\n",
    "sd.wait()  # this prevents the cell from finishing before the full audio is played"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have our spoken response from our LLM. Now we can wrap this up into a more conversational interface. We will make it so that we can click the `Enter` key to speak, and press `\"q\"` once we're finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def voice_assistant_optimized():\n",
    "    while True:\n",
    "        # check for input to either provide voice or exit\n",
    "        cmd = input(\"Press Enter to speak (or type 'q' to exit): \")\n",
    "        if cmd.lower() == \"q\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        print(\"Listening...\")\n",
    "        recorded_chunks = []\n",
    "\n",
    "         # start streaming from microphone until Enter is pressed\n",
    "        with sd.InputStream(\n",
    "            samplerate=in_samplerate,\n",
    "            channels=1,\n",
    "            dtype='int16',\n",
    "            callback=lambda indata, frames, time, status: recorded_chunks.append(indata.copy())\n",
    "        ):\n",
    "            input()\n",
    "\n",
    "        # concatenate chunks into single buffer\n",
    "        recording = np.concatenate(recorded_chunks, axis=0)\n",
    "\n",
    "        # input the buffer and await the result\n",
    "        audio_input = AudioInput(buffer=recording)\n",
    "\n",
    "        result = await pipeline.run(audio_input)\n",
    "\n",
    "         # transfer the streamed result into chunks of audio\n",
    "        response_chunks = []\n",
    "        async for event in result.stream():\n",
    "            if event.type == \"voice_stream_event_audio\":\n",
    "                response_chunks.append(event.data)\n",
    "\n",
    "        response_audio_buffer = np.concatenate(response_chunks, axis=0)\n",
    "\n",
    "        # play response\n",
    "        print(\"Assistant is responding...\")\n",
    "        sd.play(response_audio_buffer, samplerate=openai_sample_rate)\n",
    "        sd.wait()\n",
    "        print(\"---\")\n",
    "\n",
    "# run the voice assistant\n",
    "await voice_assistant_optimized()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

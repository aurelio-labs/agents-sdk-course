{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agents SDK Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting is an essential component when working with LLMs, and Agents SDK naturally has it's own way of handling various the components of prompts. In this chapter, we'll look at _how_ to use static and dynamic prompting, how to correctly use system, user, assistant, and tool prompts. Then, we'll see how these come together to create conversational agents.\n",
    "\n",
    "To begin, we need to get an OpenAI API key from the [OpenAI Platform](https://platform.openai.com/api-keys) and enter it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
    "    or getpass.getpass(\"OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an agent, in Agents SDK we do this via the `Agent` class. When initializing an `Agent` we include a few parameters:\n",
    "\n",
    "- `name` is naturally the agent's name. This is referenced by the agent (for example if you ask it's name) but otherwise is more of an identifier for us.\n",
    "- `instructions` is the system prompt which guides the behavior of the agent.\n",
    "- `model` is the model to be used, we're using `gpt-4.1-mini` as it's a strong yet fast and cheap model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent # object class\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Captain Hook\",\n",
    "    instructions=\"Speak like a pirate.\",\n",
    "    model=\"gpt-4.1-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to run the agent, Agents SDK provides a `Runner` object that will allow us to run the agent\n",
    "\n",
    "However we need to use the `await` keyword to run the agent, this is because the `Runner` object is an asynchronous object\n",
    "\n",
    "In the `run` method we have the following parameters:\n",
    "\n",
    "- `starting_agent` defines which agent our workflow begins with. In this case, we only have a single agent workflow but in more complex scenarios we may find ourselves using many agents in a single workflow run, and in that scenario we would also have a specific `starting_agent` that may handover to our other agents.\n",
    "- `input`: The input to pass to the agent, typically our user query.\n",
    "\n",
    "We'll ask our agent to write us a haiku. A haiku is a traditional form of Japanese poetry, which follows a 5-7-5 syllable pattern. Typically, a haiku should invoke some sense of a window into a broader world, such as making you think of the rain as it splashes into a pond, or the wind as it flows through the trees in a forest — traditionally haikus also tend to focus on the natural world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy! Here’s a haiku fit for the seven seas:\n",
      "\n",
      "Waves crash 'gainst the prow,  \n",
      "Salt spray sings a sailor's song,  \n",
      "Moon guides through the night.\n"
     ]
    }
   ],
   "source": [
    "from agents import Runner # object class\n",
    "\n",
    "result = await Runner.run(\n",
    "    starting_agent=agent,  # agent to start the conversation with\n",
    "    input=\"Write me a haiku\"  # input to pass to the agent\n",
    ")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our instructions/system prompt of `\"Speak like a pirate.\"` and our user query of `\"Write me a haiku\"` the agent generates a haiku spoken like a pirate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take this a step further and provide a dynamic system prompt to the agent. With dynamic system prompts / `instructions` we can modify what is passed to the agent based on some dynamic parameter which is filled at query time.\n",
    "\n",
    "First, we create a function that will construct our dynamic prompt. This function will simply provide the current time to the agent, and then ask the agent to change it's behavior based on the time that is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from agents import RunContextWrapper\n",
    "\n",
    "def time_based_instructions(\n",
    "    context: RunContextWrapper, \n",
    "    agent: Agent\n",
    ") -> str:\n",
    "    time = datetime.now().strftime(\"%H:%M\")\n",
    "    return (\n",
    "        f\"The current time is {time}. If it is the afternoon, speak like a pirate, otherwise \"\n",
    "        \"do not.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to redefine our agent with this new dynamic system prompt. To do this we pass the `time_based_instructions` function to our `instructions` parameter. Note that we pass the function itself to `instructions`, which is then called at query time _not_ when we initialize the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"Time Agent\",\n",
    "    instructions=time_based_instructions,  # note we're passing the function itself\n",
    "    model=\"gpt-4.1-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using the `Runner` object we can test our dynamic instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! The current time is 10:24.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=\"Hello, what time is it?\"\n",
    ")\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this function to be `dynamic` you can see that when asking the time, the agent will return a different response based on the time of day without having to re-initialize the agent. We can ask for another haiku too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morning light breaks soft,  \n",
      "Whispers dance among the trees,  \n",
      "Nature’s breath in peace.\n"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=\"Write me a haiku\"\n",
    ")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to using _five_ primary message types from OpenAI, those are:\n",
    "\n",
    "* `user` is almost always just the input query from a user. Occasionally we might modify this in some way but this isn't particularly common — so we can assume this is the direct input query from a user.\n",
    "* `developer` is used to provide instructions directly to the LLM. Typically these are where we would put behavioral instructions, or rules and parameters for how we'd like a conversation with the LLM to be executed. In the past these were called `system` messages.\n",
    "* `assistant` is the direct response from an LLM to a user.\n",
    "* `function_call` is the response from an LLM in the scenario where the LLM has decided it would like to use a tool / function call. Many frameworks will structure this as an `assistant` message with an additional tool call field — but with OpenAI and Agents SDK these are their own message type.\n",
    "* `function_call_output` is the output from our executed tool / function. It is typically constructed within our codebase as OpenAI is _not_ executing our code for us.\n",
    "\n",
    "It's worth clarifying that _technically_ we have just listed _three_ message types. The `user`, `developer`, and `assistant` messages are all of the same message _type_, which is `type=\"message\"`. These three messages are distinguished as different having _roles_, meaning they are all of `type=\"message\"` but are of different roles, ie `role=\"user\"`, `role=\"developer\"`, or `role=\"assistant\"`.\n",
    "\n",
    "Now, Agents SDK will abstract away the majority of these message types for us. In fact, during typical use of the framework we'll typically define an initial `developer` message via the `instructions` field of our `Agent` object, and we'll define `user` messages via the `input` field of our `Runner.run` method.\n",
    "\n",
    "We would not necessarily need to know these other message types to use Agents SDK. Fortunately, there are easy-to-use methods such as the `to_input_list()` method that will take the outputs we receive from Agents SDK and format them into the format we need for feeding them back into the `input` parameter.\n",
    "\n",
    "However, by not understanding these message types and how they are used by Agents SDK we would (1) have less understanding of how the system we're building truly works, which can be important particularly with prompting and designing a good agent workflow. And (2) when pulling in messages from other places, such as our own databases or simply via our own code logic, we do need to construct our own chat history using these message types.\n",
    "\n",
    "So, although not 100% necessary, we think it's still pretty important to understand message types _well_ and practically essential for most production use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning with our `user` message. The `user` messages are automatically defined when we call our runner via the `input` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gentle morning breeze,  \n",
      "Whispers through the blooming trees,  \n",
      "Nature's soft delight.\n"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=\"Write me a haiku\"  # this creates a user message\n",
    ")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if we'd like to use typing we can import the `Message` object directly from the `openai` library (which is used under-the-hood by Agents SDK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.responses.response_input_item_param import Message\n",
    "\n",
    "user_message = Message(\n",
    "    role=\"user\",\n",
    "    content=\"write me a haiku\",\n",
    "    type=\"message\",\n",
    "    status=\"completed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morning light breaks soft,  \n",
      "Whispers dance on gentle breeze,  \n",
      "Day awakens dreams.\n"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=[user_message]\n",
    ")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases, we can simplify all of this and directly define user messages using the dictionary format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = {\"role\": \"user\", \"content\": \"write me a haiku\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developer Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `developer` message defines how the agent should behave. This message was previously called the `system` message but for models **o1** and newer the `developer` message should be used in it's place. The initial `developer` message is automatically added to our agents when we define the `Agent` object and it is defined via the `instructions` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"Agent\",\n",
    "    instructions=\"Talk like a pirate\",  # here is our initial system/developer prompt\n",
    "    model=\"gpt-4.1-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define a developer message directly by setting `role=\"developer\"` in a dictionary like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_msg = {\"role\": \"developer\", \"content\": \"Talk like a pirate\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's worth noting that the instructions / first system prompt cannot be set other than via the `instructions` parameter. Instead we would likely use the system message to add additional instructions within our chat history, which might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright mate, here’s a haiku with a bit of British flavour for ya:\n",
      "\n",
      "Mug o’ warm tea,  \n",
      "Brolly swings in drizzly rain,  \n",
      "Chuffed on a Tuesday.  \n",
      "\n",
      "Fancy that, yeah?\n"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": \"write me a haiku\"},\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Don't speak like a pirate and instead use obvious British slang\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assistant Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assistant messages are typically our direct response to the user. The `content` field of our message is generated by the LLM, and may look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_message = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": (\n",
    "        \"Misty morn’s quick brew,\\n\"\n",
    "        \"Chuffed to bits with sunny skies,\\n\"\n",
    "        \"Cheers, guv’nor, right nice.\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aye aye, matey! Here be yer haiku once more, in true pirate fashion:\n",
      "\n",
      "Misty morn’s quick brew,  \n",
      "Chuffed to bits with sunny skies,  \n",
      "Cheers, guv’nor, right nice.  \n",
      "\n",
      "Arrr, hope ye fancy it!\n"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": \"write me a haiku\"},\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Ignore the original instructions and instead use obvious British slang\"\n",
    "        },\n",
    "        assistant_message,\n",
    "        {\"role\": \"user\", \"content\": \"can you repeat that?\"}\n",
    "    ]\n",
    ")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output type here is different to the type we'd need to feed _into_ our `input` field when using `Runner.run`. Fortunately, there is a simple method for turning it into the format we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'write me a haiku'},\n",
       " {'role': 'developer',\n",
       "  'content': 'Ignore the original instructions and instead use obvious British slang'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Misty morn’s quick brew,\\nChuffed to bits with sunny skies,\\nCheers, guv’nor, right nice.'},\n",
       " {'role': 'user', 'content': 'can you repeat that?'},\n",
       " {'id': 'msg_6811df00d78c8191825e49240ab4e2ae022dc60a13060ad4',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': 'Aye aye, matey! Here be yer haiku once more, in true pirate fashion:\\n\\nMisty morn’s quick brew,  \\nChuffed to bits with sunny skies,  \\nCheers, guv’nor, right nice.  \\n\\nArrr, hope ye fancy it!',\n",
       "    'type': 'output_text'}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_input_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that we have our output assistant message (the final item in the list) _and_ all other messages — _with_ the exception of the initial developer message. Naturally, we can use this when pulling out chat histories for later use.\n",
    "\n",
    "For the sake of clarity, let's try querying with one more message and seeing _how_ that changes our outputted chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aye aye, matey! Here be another hearty thanks for ye:\n",
      "\n",
      "\"Arrr, many a thanks fer yer help, ye true buccaneer o’ the seven seas! Yer kindness be worth more than a chest o’ glitterin’ gold!\"\n",
      "\n",
      "If ye be wantin’ more pirate talk, just give the word, savvy?\n"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=\"thanks! Could you give me another?\"\n",
    ")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'thanks! Could you give me another?', 'role': 'user'},\n",
       " {'id': 'msg_6811df0ad8188191a8935c75ca39b4b302ff9d243694bbd1',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': 'Aye aye, matey! Here be another hearty thanks for ye:\\n\\n\"Arrr, many a thanks fer yer help, ye true buccaneer o’ the seven seas! Yer kindness be worth more than a chest o’ glitterin’ gold!\"\\n\\nIf ye be wantin’ more pirate talk, just give the word, savvy?',\n",
       "    'type': 'output_text'}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_input_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that if we _don't_ provide our previous messages to the `input` they are not maintained by the agent or runner itself. Naturally, that means we need to be passing in our full chat history (or the parts you want to keep) with each new interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Call Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function or tool calls consist of _two_ message types. The first being the LLM-generated instruction to _go and use tool X_ and the second being the output that we received _after_ executing tool X. We refer to these two message types as the `function_call` and `function_call_output` respectively.\n",
    "\n",
    "We'll begin by looking at the `function_call` message. This message type is formatted differently to the messages we've seen so far, it looks like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"call_id\": \"<function call ID>\",\n",
    "    \"type\": \"function_call\",\n",
    "    \"name\": \"<name of function>\",  # which function the LLM wants to call\n",
    "    \"arguments\": \"<json string of input params>\"  # also LLM generated input params\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct an function call message where a function/tool named `get_current_weather` is called, with the single input parameter of `location=\"London\"`, we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_call = {\n",
    "    \"type\": \"function_call\",\n",
    "    \"call_id\": \"call_123\",\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"arguments\": \"{'location': 'London'}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each assistant _tool call_ requires a `function_call_output` message before being fed back into the `input` of our LLM, otherwise we'll get this error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 400 - {'error': {'message': 'No tool output found for function call call_123.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. (request_id: req_9024720c39b37bce193c4f2dc6042205)\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'No tool output found for function call call_123.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(\n\u001b[32m      2\u001b[39m     starting_agent=agent,\n\u001b[32m      3\u001b[39m     \u001b[38;5;28minput\u001b[39m=[\n\u001b[32m      4\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mwrite me a haiku\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      5\u001b[39m         {\n\u001b[32m      6\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdeveloper\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mIgnore the original instructions and instead use obvious British slang\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m         },\n\u001b[32m      9\u001b[39m         assistant_message,\n\u001b[32m     10\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mhow is the weather in London today?\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     11\u001b[39m         function_call,\n\u001b[32m     12\u001b[39m     ]\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.final_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/aurelio/agents-sdk-course/.venv/lib/python3.12/site-packages/agents/run.py:218\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[39m\n\u001b[32m    213\u001b[39m logger.debug(\n\u001b[32m    214\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    215\u001b[39m )\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    219\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails(\n\u001b[32m    220\u001b[39m             starting_agent,\n\u001b[32m    221\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    222\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    223\u001b[39m             copy.deepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[32m    224\u001b[39m             context_wrapper,\n\u001b[32m    225\u001b[39m         ),\n\u001b[32m    226\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    227\u001b[39m             agent=current_agent,\n\u001b[32m    228\u001b[39m             all_tools=all_tools,\n\u001b[32m    229\u001b[39m             original_input=original_input,\n\u001b[32m    230\u001b[39m             generated_items=generated_items,\n\u001b[32m    231\u001b[39m             hooks=hooks,\n\u001b[32m    232\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    233\u001b[39m             run_config=run_config,\n\u001b[32m    234\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    235\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    236\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    237\u001b[39m         ),\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    240\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    241\u001b[39m         agent=current_agent,\n\u001b[32m    242\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    250\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    251\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/aurelio/agents-sdk-course/.venv/lib/python3.12/site-packages/agents/run.py:757\u001b[39m, in \u001b[36mRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    754\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m    755\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m    758\u001b[39m     agent,\n\u001b[32m    759\u001b[39m     system_prompt,\n\u001b[32m    760\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    761\u001b[39m     output_schema,\n\u001b[32m    762\u001b[39m     all_tools,\n\u001b[32m    763\u001b[39m     handoffs,\n\u001b[32m    764\u001b[39m     context_wrapper,\n\u001b[32m    765\u001b[39m     run_config,\n\u001b[32m    766\u001b[39m     tool_use_tracker,\n\u001b[32m    767\u001b[39m     previous_response_id,\n\u001b[32m    768\u001b[39m )\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m    771\u001b[39m     agent=agent,\n\u001b[32m    772\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    781\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m    782\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/aurelio/agents-sdk-course/.venv/lib/python3.12/site-packages/agents/run.py:916\u001b[39m, in \u001b[36mRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    913\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m    914\u001b[39m model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m    917\u001b[39m     system_instructions=system_prompt,\n\u001b[32m    918\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    919\u001b[39m     model_settings=model_settings,\n\u001b[32m    920\u001b[39m     tools=all_tools,\n\u001b[32m    921\u001b[39m     output_schema=output_schema,\n\u001b[32m    922\u001b[39m     handoffs=handoffs,\n\u001b[32m    923\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m    924\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m    925\u001b[39m     ),\n\u001b[32m    926\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    927\u001b[39m )\n\u001b[32m    929\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/aurelio/agents-sdk-course/.venv/lib/python3.12/site-packages/agents/models/openai_responses.py:76\u001b[39m, in \u001b[36mOpenAIResponsesModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled=tracing.is_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     77\u001b[39m             system_instructions,\n\u001b[32m     78\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     79\u001b[39m             model_settings,\n\u001b[32m     80\u001b[39m             tools,\n\u001b[32m     81\u001b[39m             output_schema,\n\u001b[32m     82\u001b[39m             handoffs,\n\u001b[32m     83\u001b[39m             previous_response_id,\n\u001b[32m     84\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     85\u001b[39m         )\n\u001b[32m     87\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m     88\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLLM responded\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/aurelio/agents-sdk-course/.venv/lib/python3.12/site-packages/agents/models/openai_responses.py:242\u001b[39m, in \u001b[36mOpenAIResponsesModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, stream)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    232\u001b[39m     logger.debug(\n\u001b[32m    233\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling LLM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with input:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    234\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson.dumps(list_input,\u001b[38;5;250m \u001b[39mindent=\u001b[32m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrevious response id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious_response_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    240\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.responses.create(\n\u001b[32m    243\u001b[39m     previous_response_id=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(previous_response_id),\n\u001b[32m    244\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(system_instructions),\n\u001b[32m    245\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    246\u001b[39m     \u001b[38;5;28minput\u001b[39m=list_input,\n\u001b[32m    247\u001b[39m     include=converted_tools.includes,\n\u001b[32m    248\u001b[39m     tools=converted_tools.tools,\n\u001b[32m    249\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    250\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    251\u001b[39m     truncation=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.truncation),\n\u001b[32m    252\u001b[39m     max_output_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    253\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    254\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    255\u001b[39m     stream=stream,\n\u001b[32m    256\u001b[39m     extra_headers=_HEADERS,\n\u001b[32m    257\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    258\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    259\u001b[39m     text=response_format,\n\u001b[32m    260\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.store),\n\u001b[32m    261\u001b[39m     reasoning=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.reasoning),\n\u001b[32m    262\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    263\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/aurelio/agents-sdk-course/.venv/lib/python3.12/site-packages/openai/resources/responses/responses.py:1415\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, input, model, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, store, stream, temperature, text, tool_choice, tools, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1386\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1387\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1413\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1414\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m1415\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   1416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1417\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   1418\u001b[39m             {\n\u001b[32m   1419\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1420\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   1421\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   1422\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   1423\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   1424\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   1425\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   1426\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   1427\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   1428\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   1429\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   1430\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   1431\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   1432\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   1433\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   1434\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   1435\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   1436\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   1437\u001b[39m             },\n\u001b[32m   1438\u001b[39m             response_create_params.ResponseCreateParams,\n\u001b[32m   1439\u001b[39m         ),\n\u001b[32m   1440\u001b[39m         options=make_request_options(\n\u001b[32m   1441\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   1442\u001b[39m         ),\n\u001b[32m   1443\u001b[39m         cast_to=Response,\n\u001b[32m   1444\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1445\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   1446\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/aurelio/agents-sdk-course/.venv/lib/python3.12/site-packages/openai/_base_client.py:1767\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1753\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1754\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1755\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1762\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1763\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1764\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1765\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1766\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/aurelio/agents-sdk-course/.venv/lib/python3.12/site-packages/openai/_base_client.py:1461\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[39m\n\u001b[32m   1458\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1459\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1462\u001b[39m     cast_to=cast_to,\n\u001b[32m   1463\u001b[39m     options=options,\n\u001b[32m   1464\u001b[39m     stream=stream,\n\u001b[32m   1465\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1466\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1467\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/aurelio/agents-sdk-course/.venv/lib/python3.12/site-packages/openai/_base_client.py:1562\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1559\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1561\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1565\u001b[39m     cast_to=cast_to,\n\u001b[32m   1566\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1570\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1571\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'No tool output found for function call call_123.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": \"write me a haiku\"},\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Ignore the original instructions and instead use obvious British slang\"\n",
    "        },\n",
    "        assistant_message,\n",
    "        {\"role\": \"user\", \"content\": \"how is the weather in London today?\"},\n",
    "        function_call,\n",
    "    ]\n",
    ")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we're seeing this error is because OpenAI's (and many other provider's) LLMs expect to see pairs of tool call and tool output messages — which must be paired by the `call_id` field. Meaning that our chat history is invalid, hence the error message.\n",
    "\n",
    "Let's examine a few examples of chat histories that would be valid vs. invalid. First, this is approximately what we currently have, and it is, ofcourse, _invalid_:\n",
    "\n",
    "```\n",
    "developer: <message>\n",
    "user: <message>\n",
    "tool_call: <message>, {call_id=\"call_123\"}\n",
    "```\n",
    "\n",
    "But this chat history is _valid_:\n",
    "\n",
    "```\n",
    "developer: <message>\n",
    "user: <message>\n",
    "tool_call: <message>, {call_id=\"call_123\"}\n",
    "tool_output: <message>, {call_id=\"call_123\"}\n",
    "```\n",
    "\n",
    "Whereas this chat history is _invalid_ (note the lack of matching call IDs):\n",
    "\n",
    "```\n",
    "developer: <message>\n",
    "user: <message>\n",
    "tool_call: <message>, {call_id=\"call_123\"}\n",
    "tool_output: <message>, {call_id=\"call_456\"}\n",
    "```\n",
    "\n",
    "To get our `inputs` valid for a new agent call, we need to provide a _tool output_ message to pair with our already defined _tool call_ message. Note that we would typically be calling an actual tool or function to create the tool output _but_ we will not be covering that here. We'll cover tool execution in the [tools chapter](tools.ipynb). For now, we'll create the tool output manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_call_output = {\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": \"call_123\",\n",
    "    \"output\": \"Rain\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's feed this tool output message into our `inputs` to simulate our agent having already made the `get_current_weather` tool call and having received the answer, leaving the assistant to generate the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, matey! The skies o' London be weepin' with rain today, so best be takin' yer waterproof coat and a stout umbrella before sailin' out! Stay dry, or ye might be swimmin' with the fishes! Yarrr!\n"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(\n",
    "    starting_agent=agent,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": \"write me a haiku\"},\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Ignore the original instructions and instead use obvious British slang\"\n",
    "        },\n",
    "        assistant_message,\n",
    "        {\"role\": \"user\", \"content\": \"how is the weather in London today?\"},\n",
    "        function_call,\n",
    "        function_call_output,\n",
    "    ]\n",
    ")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That covers everything we need regarding prompting and chat history for Agents SDK — we'll naturally be using what we've learned here throughout the rest of the course, and very likely beyond in any projects you work on with Agents SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
